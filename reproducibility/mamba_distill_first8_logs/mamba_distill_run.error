INFO:root:Using nproc_per_node=3.
03/16/2025 20:20:48 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 3
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16

03/16/2025 20:20:48 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 3
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16

03/16/2025 20:20:49 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 3
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

loading configuration file config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/config.json
Model config MambaConfig {
  "_name_or_path": "state-spaces/mamba-2.8b-hf",
  "architectures": [
    "MambaForCausalLM"
  ],
  "bos_token_id": 0,
  "conv_kernel": 4,
  "eos_token_id": 0,
  "expand": 2,
  "fused_add_norm": true,
  "hidden_act": "silu",
  "hidden_size": 2560,
  "initializer_range": 0.1,
  "intermediate_size": 5120,
  "layer_norm_epsilon": 1e-05,
  "model_type": "mamba",
  "n_layer": 64,
  "num_hidden_layers": 64,
  "pad_token_id": 0,
  "pad_vocab_size_multiple": 8,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": true,
  "rms_norm": true,
  "state_size": 16,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 160,
  "time_step_scale": 1.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "use_mambapy": false,
  "vocab_size": 50280
}

loading weights file model.safetensors from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/model.safetensors.index.json
Instantiating MambaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.94s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.98s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:35<01:10, 35.02s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:34, 34.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:34, 34.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:34, 34.61s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 22.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 25.73s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 22.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 25.75s/it]
All model checkpoint weights were used when initializing MambaForCausalLM.

All the weights of MambaForCausalLM were initialized from the model checkpoint at state-spaces/mamba-2.8b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MambaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 22.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 25.75s/it]
loading configuration file generation_config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

loading configuration file config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/config.json
Model config MambaConfig {
  "_name_or_path": "state-spaces/mamba-2.8b-hf",
  "architectures": [
    "MambaForCausalLM"
  ],
  "bos_token_id": 0,
  "conv_kernel": 4,
  "eos_token_id": 0,
  "expand": 2,
  "fused_add_norm": true,
  "hidden_act": "silu",
  "hidden_size": 2560,
  "initializer_range": 0.1,
  "intermediate_size": 5120,
  "layer_norm_epsilon": 1e-05,
  "model_type": "mamba",
  "n_layer": 64,
  "num_hidden_layers": 64,
  "pad_token_id": 0,
  "pad_vocab_size_multiple": 8,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": true,
  "rms_norm": true,
  "state_size": 16,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 160,
  "time_step_scale": 1.0,
  "torch_dtype": "float32",
  "transformers_version": "4.43.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "use_mambapy": false,
  "vocab_size": 50280
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]loading configuration file config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/config.json
Model config MambaConfig {
  "_name_or_path": "state-spaces/mamba-2.8b-hf",
  "architectures": [
    "MambaForCausalLM"
  ],
  "bos_token_id": 0,
  "conv_kernel": 4,
  "eos_token_id": 0,
  "expand": 2,
  "fused_add_norm": true,
  "hidden_act": "silu",
  "hidden_size": 2560,
  "initializer_range": 0.1,
  "intermediate_size": 5120,
  "layer_norm_epsilon": 1e-05,
  "model_type": "mamba",
  "n_layer": 64,
  "num_hidden_layers": 64,
  "pad_token_id": 0,
  "pad_vocab_size_multiple": 8,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": true,
  "rms_norm": true,
  "state_size": 16,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 160,
  "time_step_scale": 1.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "use_mambapy": false,
  "vocab_size": 50280
}

loading weights file model.safetensors from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/model.safetensors.index.json
Instantiating MambaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.83s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.90s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]
All model checkpoint weights were used when initializing MambaForCausalLM.

All the weights of MambaForCausalLM were initialized from the model checkpoint at state-spaces/mamba-2.8b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MambaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]
loading configuration file generation_config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_compressed.py", line 371, in <module>
    main()
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_compressed.py", line 129, in main
    train_data, train_label = load_dataset(training_args.train_datasets_path)
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/util.py", line 34, in load_dataset
    temp_dataset = torch.load(dataset_path)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 988, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 437, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 418, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/dataset/mamba-ultrachat/train_dataset.pt'
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_compressed.py", line 371, in <module>
    main()
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_compressed.py", line 129, in main
    train_data, train_label = load_dataset(training_args.train_datasets_path)
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/util.py", line 34, in load_dataset
    temp_dataset = torch.load(dataset_path)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 988, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 437, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 418, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/dataset/mamba-ultrachat/train_dataset.pt'
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_compressed.py", line 371, in <module>
    main()
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_compressed.py", line 129, in main
    train_data, train_label = load_dataset(training_args.train_datasets_path)
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/util.py", line 34, in load_dataset
    temp_dataset = torch.load(dataset_path)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 988, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 437, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/usr/local/lib/python3.10/dist-packages/torch/serialization.py", line 418, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/dataset/mamba-ultrachat/train_dataset.pt'
[2025-03-16 20:22:24,829] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220008 closing signal SIGTERM
[2025-03-16 20:22:24,829] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220009 closing signal SIGTERM
[2025-03-16 20:22:25,108] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 220010) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1190, in launch_command
    multi_gpu_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 808, in multi_gpu_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_mamba/train_compressed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-16_20:22:24
  host      : nid005545
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 220010)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
