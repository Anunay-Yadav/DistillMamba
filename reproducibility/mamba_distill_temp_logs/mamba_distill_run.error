INFO:root:Using nproc_per_node=3.
03/16/2025 11:43:01 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 3
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

03/16/2025 11:43:01 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 3
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16

03/16/2025 11:43:01 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 3
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16

loading configuration file config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/config.json
Model config MambaConfig {
  "_name_or_path": "state-spaces/mamba-2.8b-hf",
  "architectures": [
    "MambaForCausalLM"
  ],
  "bos_token_id": 0,
  "conv_kernel": 4,
  "eos_token_id": 0,
  "expand": 2,
  "fused_add_norm": true,
  "hidden_act": "silu",
  "hidden_size": 2560,
  "initializer_range": 0.1,
  "intermediate_size": 5120,
  "layer_norm_epsilon": 1e-05,
  "model_type": "mamba",
  "n_layer": 64,
  "num_hidden_layers": 64,
  "pad_token_id": 0,
  "pad_vocab_size_multiple": 8,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": true,
  "rms_norm": true,
  "state_size": 16,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 160,
  "time_step_scale": 1.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "use_mambapy": false,
  "vocab_size": 50280
}

loading weights file model.safetensors from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/model.safetensors.index.json
Instantiating MambaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:36<01:12, 36.42s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:36<01:13, 36.52s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:36<01:13, 36.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:34, 34.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:34, 34.72s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:09<00:34, 34.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 22.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 25.78s/it]
All model checkpoint weights were used when initializing MambaForCausalLM.

All the weights of MambaForCausalLM were initialized from the model checkpoint at state-spaces/mamba-2.8b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MambaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 22.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 25.78s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 22.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:17<00:00, 25.79s/it]
loading configuration file generation_config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

loading configuration file config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/config.json
Model config MambaConfig {
  "_name_or_path": "state-spaces/mamba-2.8b-hf",
  "architectures": [
    "MambaForCausalLM"
  ],
  "bos_token_id": 0,
  "conv_kernel": 4,
  "eos_token_id": 0,
  "expand": 2,
  "fused_add_norm": true,
  "hidden_act": "silu",
  "hidden_size": 2560,
  "initializer_range": 0.1,
  "intermediate_size": 5120,
  "layer_norm_epsilon": 1e-05,
  "model_type": "mamba",
  "n_layer": 64,
  "num_hidden_layers": 64,
  "pad_token_id": 0,
  "pad_vocab_size_multiple": 8,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": true,
  "rms_norm": true,
  "state_size": 16,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 160,
  "time_step_scale": 1.0,
  "torch_dtype": "float32",
  "transformers_version": "4.43.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "use_mambapy": false,
  "vocab_size": 50280
}

loading file vocab.json from cache at None
loading file merges.txt from cache at None
loading file tokenizer.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]loading configuration file config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/config.json
Model config MambaConfig {
  "_name_or_path": "state-spaces/mamba-2.8b-hf",
  "architectures": [
    "MambaForCausalLM"
  ],
  "bos_token_id": 0,
  "conv_kernel": 4,
  "eos_token_id": 0,
  "expand": 2,
  "fused_add_norm": true,
  "hidden_act": "silu",
  "hidden_size": 2560,
  "initializer_range": 0.1,
  "intermediate_size": 5120,
  "layer_norm_epsilon": 1e-05,
  "model_type": "mamba",
  "n_layer": 64,
  "num_hidden_layers": 64,
  "pad_token_id": 0,
  "pad_vocab_size_multiple": 8,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": true,
  "rms_norm": true,
  "state_size": 16,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 160,
  "time_step_scale": 1.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "use_mambapy": false,
  "vocab_size": 50280
}

loading weights file model.safetensors from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/model.safetensors.index.json
Instantiating MambaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.92s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.99s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.87s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.92s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]
All model checkpoint weights were used when initializing MambaForCausalLM.

All the weights of MambaForCausalLM were initialized from the model checkpoint at state-spaces/mamba-2.8b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MambaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]
loading configuration file generation_config.json from cache at /users/anunay/.cache/huggingface/hub/models--state-spaces--mamba-2.8b-hf/snapshots/96c48e0292b63f5346b6d30061af2551f7101e26/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0,
  "pad_token_id": 0
}

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_mamba_compressed.py", line 368, in <module>
    main()
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/train_mamba/train_mamba_compressed.py", line 339, in main
    student_outputs = student_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1521, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1357, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/mamba/hybrid_wrapper.py", line 185, in forward
    return self.model(input_ids, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/mamba/modeling_mamba.py", line 757, in forward
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 3055, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 1 has a total capacty of 94.50 GiB of which 3.58 GiB is free. Including non-PyTorch memory, this process has 62.08 GiB memory in use. Of the allocated memory 60.47 GiB is allocated by PyTorch, and 196.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Currently logged in as: anunay-yadav (anunay-yadav-epfl) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/wandb/run-20250316_114521-twd5fzdx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-bush-31
wandb: ⭐️ View project at https://wandb.ai/anunay-yadav-epfl/mamba_distill
wandb: 🚀 View run at https://wandb.ai/anunay-yadav-epfl/mamba_distill/runs/twd5fzdx
03/16/2025 11:45:22 - INFO - __main__ - ***** Running training *****
03/16/2025 11:45:22 - INFO - __main__ -   Num examples = 1468352
03/16/2025 11:45:22 - INFO - __main__ -   Num Epochs = 1
03/16/2025 11:45:22 - INFO - __main__ -   Instantaneous batch size per device = 8
03/16/2025 11:45:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 96
03/16/2025 11:45:22 - INFO - __main__ -   Gradient Accumulation steps = 4
03/16/2025 11:45:22 - INFO - __main__ -   Total optimization steps = 15296
  0%|          | 0/15296 [00:00<?, ?it/s][2025-03-16 11:45:23,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 194387 closing signal SIGTERM
[2025-03-16 11:45:23,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 194389 closing signal SIGTERM
[2025-03-16 11:45:24,384] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 194388) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1190, in launch_command
    multi_gpu_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 808, in multi_gpu_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_mamba/train_mamba_compressed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-16_11:45:23
  host      : nid006983
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 194388)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
