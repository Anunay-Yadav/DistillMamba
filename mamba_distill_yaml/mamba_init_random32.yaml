model_name: state-spaces/mamba-2.8b-hf 
layers_to_copy: [2, 4, 5, 7, 8, 10, 16, 17, 19, 20, 21, 28, 29, 32, 33, 34, 37, 41, 42, 44, 45, 46, 49, 51, 52, 54, 56, 57, 58, 60, 61, 63]
number_of_layers: 32
kl_weight: 0.1
ce_weight: 1
do_eval: true
run_name: mamba_init_random32
eval_datasets_path: [/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/dataset/mamba-ultrachat/eval_dataset.pt]
train_datasets_path: [/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/dataset/mamba-ultrachat/train_dataset.pt]
output_dir: output/mamba_init_random32/
lm_eval_tasks: mmlu,hellaswag,piqa,arc_easy,arc_challenge,winogrande,openbookqa,race
seed: 42
save_steps: 5000
warmup_steps: 1500
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
num_train_epochs: 0
gradient_accumulation_steps: 4
lr_scheduler_type: cosine
learning_rate: 4.0e-5
max_grad_norm: 0.1
head_dim: None