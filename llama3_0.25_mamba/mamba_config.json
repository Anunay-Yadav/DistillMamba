{
    "d_model": 4096,
    "ssm_cfg": {
        "expand": 1
    },
    "rms_norm_eps": 1e-05,
    "vocab_size": null,
    "d_inner": 4096,
    "d_xb": 1024,
    "intermediate_size": 14336,
    "hidden_act": "silu",
    "n_layer": 32,
    "attn_layers": [
        1,
        2,
        3,
        5,
        6,
        7,
        9,
        10,
        11,
        13,
        14,
        15,
        17,
        18,
        19,
        21,
        22,
        23,
        25,
        26,
        27,
        29,
        30,
        31
    ]
}