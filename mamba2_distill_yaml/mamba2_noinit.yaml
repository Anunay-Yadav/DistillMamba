model_name: mamba2-2.7B
ssm_layers: [0, 4, 8, 12, 16, 20, 24, 28]
kl_weight: 0.1
ce_weight: 1
do_eval: true
eval_datasets_path: [/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/dataset/mamba-ultrachat/eval_dataset.pt]
train_datasets_path: [/iopsstor/scratch/cscs/anunay/my_repo/DistillMamba/dataset/mamba-ultrachat/train_dataset.pt]
output_dir: output/mamba2_no_init/
seed: 42
save_steps: 5000
warmup_steps: 1500
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
num_train_epochs: 1
gradient_accumulation_steps: 4
lr_scheduler_type: cosine
learning_rate: 8.0e-5
max_grad_norm: 0.1
head_dim: None